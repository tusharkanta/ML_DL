{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S5_Step5_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusharkanta/ML_DL/blob/eva/S5/S5_Step5_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scXmaoqkTEoV",
        "colab_type": "text"
      },
      "source": [
        "Target:\n",
        "\n",
        "Kept the model same as previous as the parameters has hit 9904. Reduced batch size from 32 to 24, changed learning rate from 0.01 to 0.008 and changed random roation param back to -7,7. Result: Parameters: 9,904 Best Train Accuracy: 98.86 Best Test Accuracy: 99.42 Analysis: Validation accuracy has improved and remained quite consistent (does not fluctuate rapidly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um",
        "colab_type": "text"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n",
        "\n",
        "Here is the list of all the transformations which come pre-built with PyTorch\n",
        "\n",
        "1.   Compose\n",
        "2.   ToTensor\n",
        "3.   ToPILImage\n",
        "4. Normalize\n",
        "5. Resize\n",
        "6. Scale\n",
        "7. CenterCrop\n",
        "8. Pad\n",
        "9. Lambda\n",
        "10. RandomApply\n",
        "11. RandomChoice\n",
        "12. RandomOrder\n",
        "13. RandomCrop\n",
        "14. RandomHorizontalFlip\n",
        "15. RandomVerticalFlip\n",
        "16. RandomResizedCrop\n",
        "17. RandomSizedCrop\n",
        "18. FiveCrop\n",
        "19. TenCrop\n",
        "20. LinearTransformation\n",
        "21. ColorJitter\n",
        "22. RandomRotation\n",
        "23. RandomAffine\n",
        "24. Grayscale\n",
        "25. RandomGrayscale\n",
        "26. RandomPerspective\n",
        "27. RandomErasing\n",
        "\n",
        "You can read more about them [here](https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                       #transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab_type": "code",
        "outputId": "24607cad-33ca-4180-dde5-a6705f237afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=24, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=16)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TFjoFekE_va",
        "colab_type": "text"
      },
      "source": [
        "# Data Statistics\n",
        "\n",
        "It is important to know your data very well. Let's check some of the statistics around our data and how it actually looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWZPPo3yEHDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l9lNaWYKuik",
        "colab_type": "text"
      },
      "source": [
        "## MORE\n",
        "\n",
        "It is important that we view as many images as possible. This is required to get some idea on image augmentation later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXXAg8hbK16u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF5-8_I3G6ix",
        "colab_type": "text"
      },
      "source": [
        "# How did we get those mean and std values which we used above?\n",
        "\n",
        "Let's run a small experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yooPHm_aFc5A",
        "colab_type": "code",
        "outputId": "e5c53200-a482-47ba-8995-79d8011c697f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# simple transform\n",
        "simple_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       \n",
        "                                       transforms.ToTensor(),\n",
        "                                      #  transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "exp = datasets.MNIST('./data', train=True, download=True, transform=simple_transforms)\n",
        "exp_data = exp.train_data\n",
        "exp_data = exp.transform(exp_data.numpy())\n",
        "\n",
        "print('[Train]')\n",
        "print(' - Numpy Shape:', exp.train_data.cpu().numpy().shape)\n",
        "print(' - Tensor Shape:', exp.train_data.size())\n",
        "print(' - min:', torch.min(exp_data))\n",
        "print(' - max:', torch.max(exp_data))\n",
        "print(' - mean:', torch.mean(exp_data))\n",
        "print(' - std:', torch.std(exp_data))\n",
        "print(' - var:', torch.var(exp_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Train]\n",
            " - Numpy Shape: (60000, 28, 28)\n",
            " - Tensor Shape: torch.Size([60000, 28, 28])\n",
            " - min: tensor(0.)\n",
            " - max: tensor(1.)\n",
            " - mean: tensor(0.1305)\n",
            " - std: tensor(0.3081)\n",
            " - var: tensor(0.0949)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h",
        "colab_type": "text"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 26*26*16 RF=3\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 24*24*16  RF=5\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "         # output_size = 24\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 24*24*8  RF=5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12*12*8  RF=6\n",
        "        \n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 10*10*16  RF=10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 8*8*16  RF=14\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=24, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 6*6*24  RF=18\n",
        "        #self.convblock7 = nn.Sequential(\n",
        "         #   nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "          #  nn.ReLU(),            \n",
        "           # nn.BatchNorm2d(16),\n",
        "            #nn.Dropout(dropout_value)\n",
        "        #) # output_size = 6\n",
        "        \n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6)\n",
        "        ) # output_size = 1*1*24  RF=28\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=24, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(10),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        ) # output_size = 1*1*10  RF=28\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        #x = self.convblock7(x)\n",
        "        x = self.gap(x)        \n",
        "        x = self.convblock8(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo",
        "colab_type": "text"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is. \n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab_type": "code",
        "outputId": "4e122f12-68d7-4dac-b567-bfa3b752f8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             144\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "           Dropout-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           2,304\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "           Dropout-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9            [-1, 8, 24, 24]             128\n",
            "        MaxPool2d-10            [-1, 8, 12, 12]               0\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,152\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "          Dropout-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-16             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 16, 8, 8]              32\n",
            "          Dropout-18             [-1, 16, 8, 8]               0\n",
            "           Conv2d-19             [-1, 24, 6, 6]           3,456\n",
            "             ReLU-20             [-1, 24, 6, 6]               0\n",
            "      BatchNorm2d-21             [-1, 24, 6, 6]              48\n",
            "          Dropout-22             [-1, 24, 6, 6]               0\n",
            "        AvgPool2d-23             [-1, 24, 1, 1]               0\n",
            "           Conv2d-24             [-1, 10, 1, 1]             240\n",
            "================================================================\n",
            "Total params: 9,904\n",
            "Trainable params: 9,904\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.76\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.80\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "All right, so we have 6.3M params, and that's too many, we know that. But the purpose of this notebook is to set things right for our future experiments. \n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs. \n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq",
        "colab_type": "text"
      },
      "source": [
        "# Let's Train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCFxeAKOB53",
        "colab_type": "code",
        "outputId": "e336d718-40de-4988-cfa7-a2a798a8f4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.008, momentum=0.9)\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.5230476260185242 Batch_id=2499 Accuracy=93.47: 100%|██████████| 2500/2500 [00:52<00:00, 47.86it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0565, Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.048945147544145584 Batch_id=2499 Accuracy=97.49: 100%|██████████| 2500/2500 [00:51<00:00, 48.21it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0360, Accuracy: 9886/10000 (98.86%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.006156543735414743 Batch_id=2499 Accuracy=97.98: 100%|██████████| 2500/2500 [00:51<00:00, 48.50it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.00901921559125185 Batch_id=2499 Accuracy=98.23: 100%|██████████| 2500/2500 [00:51<00:00, 48.62it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 9905/10000 (99.05%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.031010866165161133 Batch_id=2499 Accuracy=98.36: 100%|██████████| 2500/2500 [00:50<00:00, 49.46it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0286, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.03724147006869316 Batch_id=2499 Accuracy=98.38: 100%|██████████| 2500/2500 [00:51<00:00, 48.26it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0298, Accuracy: 9916/10000 (99.16%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.015348036773502827 Batch_id=2499 Accuracy=98.56: 100%|██████████| 2500/2500 [00:52<00:00, 47.72it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0267, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0015826026210561395 Batch_id=2499 Accuracy=98.58: 100%|██████████| 2500/2500 [00:52<00:00, 50.66it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0240, Accuracy: 9925/10000 (99.25%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0012954274425283074 Batch_id=2499 Accuracy=98.67: 100%|██████████| 2500/2500 [00:51<00:00, 48.12it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0198, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.007194161415100098 Batch_id=2499 Accuracy=98.67: 100%|██████████| 2500/2500 [00:52<00:00, 47.93it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0007086594705469906 Batch_id=2499 Accuracy=98.70: 100%|██████████| 2500/2500 [00:51<00:00, 48.57it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0019975900650024414 Batch_id=2499 Accuracy=98.74: 100%|██████████| 2500/2500 [00:50<00:00, 49.48it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0011540254345163703 Batch_id=2499 Accuracy=98.79: 100%|██████████| 2500/2500 [00:51<00:00, 52.67it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0215, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.027484416961669922 Batch_id=2499 Accuracy=98.79: 100%|██████████| 2500/2500 [00:51<00:00, 48.55it/s]\n",
            "  0%|          | 0/2500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0064284005202353 Batch_id=2499 Accuracy=98.86: 100%|██████████| 2500/2500 [00:51<00:00, 48.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0202, Accuracy: 9941/10000 (99.41%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RaqGSEOWDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odozjbIvY12p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}