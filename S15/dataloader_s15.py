# -*- coding: utf-8 -*-
"""dataloader_s15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vCtGSsb2gs81CkExpRnOTpnnEFsiwdlP
"""

# -*- coding: utf-8 -*-
"""dataloader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AflSpxgdQ42WJXLilCNWW1KTal3MrqS3
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
import torchvision
from zipfile import ZipFile
import requests
from io import StringIO, BytesIO
import torchvision.transforms as transform
import matplotlib.pyplot as plt
import numpy as np
import cv2
from torchvision.datasets.folder import default_loader
from torchvision.datasets.vision import VisionDataset
import os
from PIL import Image
import albumentations as alb
from albumentations.pytorch import ToTensor
from torch.utils.data import Dataset
import io,glob,os,time,random
from shutil import move
from os.path import join
from os import listdir,rmdir
import torch.utils.data as data
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torchvision.transforms import transforms


class album_compose_train:
  def __init__(self):
     
        #meandata, stddata=album_calculate_dataset_mean_std()

        #channel_mean=(meandata[0]+meandata[1]+meandata[2])/3.0
        #print('channel mean',channel_mean)
        self.albtransform = alb.Compose([
        
        alb.PadIfNeeded(value=[4,4,4]),
        alb.RandomCrop(64,64),
        alb.Rotate(limit=10),
        alb.ShiftScaleRotate(),
        alb.HorizontalFlip(p=0.5),
        alb.Cutout(num_holes=1,max_h_size=16, max_w_size=16,fill_value=0.5*255),
        alb.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ToTensor(),
        ])
 
  def __call__(self,img):
    img=np.array(img)
    img=self.albtransform(image=img)
    return img['image']

class album_compose_test:
  def __init__(self):
        #meandata, stddata=album_calculate_dataset_mean_std()
        self.albtransform = alb.Compose([
        
        #print('meandata[0]:',meandata[0]),
        alb.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ToTensor(),
        ])
 
  def __call__(self,img):
    img=np.array(img)
    img=self.albtransform(image=img)
    return img['image']


def extract_images(zipfilename):
  #gc.enable()
  #gc.get_threshold()
  #if(os.path.isdir('tiny-imagenet-200.zip')):
   #   print('Images already downloaded...')
   #   return
  zipObj = ZipFile(zipfilename, 'r')
  #print("zipObj " +zipObj)
  zipObj.extractall('./')
  zipObj.close()


def make_dataset(root: str) -> list:
    """Reads a directory with data.
    Returns a dataset as a list of tuples of paired image paths: (rgb_path, gt_path)
    """
    dataset = []

    # Our dir names
    rgb_dir = 'bg_fg_1'
    bg_dir = 'bg'
    gt_dir = 'depthMap'
    gt_dir2 = 'bg_fg_mask_1'

    # Get all the filenames from RGB folder
    rgb_fnames = sorted(os.listdir(os.path.join(root, rgb_dir)))

    # Compare file names from GT folder to file names from RGB:
    for gt_fname in sorted(os.listdir(os.path.join(root, gt_dir))):

        if gt_fname in rgb_fnames:
            # if we have a match - create pair of full path to the corresponding images
            rgb_path = os.path.join(root, rgb_dir, gt_fname)
            bgfilename = gt_fname.split('_')[0] + '_' + gt_fname.split('_')[1] + '.jpg'
            bg_path = os.path.join(root, bg_dir, bgfilename)
            # print(bg_path)
            gt_path = os.path.join(root, gt_dir, gt_fname)
            gt_path2 = os.path.join(root, gt_dir2, gt_fname)
            item = (rgb_path, bg_path, gt_path, gt_path2)
            # append to the list dataset
            dataset.append(item)
        else:
            continue

    return dataset

from torchvision.datasets.folder import make_dataset as make_dataset_original
root = '/content'
rgb = os.path.join(root, 'bg_fg_1')
bg = os.path.join(root, 'bg')
gt = os.path.join(root, 'depthMap')
gt2 = os.path.join(root, 'bg_fg_mask_1')
dirs = [root, rgb, bg, gt, gt2]

for dir_ in dirs:
    if not os.path.exists(dir_):
        os.makedirs(dir_)

#dataset_original = make_dataset_original(root, {'RGB': 0, 'GT': 1}, extensions='png')
dataset = make_dataset(root)

#print('Original make_dataset:')
#print(*dataset_original, sep='\n')

print('Our make_dataset:')
print(*dataset, sep='\n')


class CustomVisionDataset(VisionDataset):

    def __init__(self,
                 root,
                 loader=default_loader,
                 rgb_transform=None,
                 bg_transform=None,
                 gt_transform=None,
                 gt_transform2=None):
        super().__init__(root,
                         transform=rgb_transform,
                         target_transform=gt_transform
                         )

        # Prepare dataset
        samples = make_dataset(self.root)

        self.loader = loader
        self.samples = samples
        # list of RGB images
        self.rgb_samples = [s[1] for s in samples]
        self.bg_samples = [s[1] for s in samples]
        # list of GT images
        self.gt_samples = [s[1] for s in samples]
        # list of GT images 2
        self.gt_samples2 = [s[1] for s in samples]

    def __getitem__(self, index):
        """Returns a data sample from our dataset.
        """
        # getting our paths to images
        rgb_path, bg_path, gt_path, gt_path2 = self.samples[index]

        # import each image using loader (by default it's PIL)
        rgb_sample = self.loader(rgb_path)
        bg_sample = self.loader(bg_path)
        gt_sample = self.loader(gt_path)
        gt_sample2 = self.loader(gt_path2)
        # here goes tranforms if needed
        # maybe we need different tranforms for each type of image
        if self.transform is not None:
            rgb_sample = self.transform(rgb_sample)
        if self.transform is not None:
            bg_sample = self.transform(bg_sample)
        if self.target_transform is not None:
            gt_sample = self.target_transform(gt_sample)
        if self.target_transform is not None:
            gt_sample2 = self.target_transform(gt_sample2)

            # now we return the right imported pair of images (tensors)
        return rgb_sample, bg_sample, gt_sample, gt_sample2

    def __len__(self):
        return len(self.samples)

def load_data(batch=16):
    bs = batch  # batch size
    transforms = transform.ToTensor()

    shuffle = True

    dataset = CustomVisionDataset('/content', rgb_transform=transforms, bg_transform=transforms,
                                  gt_transform=transforms, gt_transform2=transforms)
    train_size = int(0.7 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=shuffle)
    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=shuffle)
    return train_loader, test_loader


def album_calculate_dataset_mean_std():

    trainset = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=ToTensor())

    testset = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=ToTensor())



    data = np.concatenate([trainset.data, testset.data], axis=0)

    data = data.astype(np.float32)/255.



    print("\nTotal dataset(train+test) shape: ", data.shape)



    means = []

    stdevs = []

    for i in range(3): # 3 channels

        pixels = data[:,:,:,i].ravel()

        means.append(np.mean(pixels))

        stdevs.append(np.std(pixels))



    return [means[0], means[1], means[2]], [stdevs[0], stdevs[1], stdevs[2]]